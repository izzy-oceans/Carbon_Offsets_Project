{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To scrape data from the retired tab from the BC offsets registry, use code below. Note: there are 65 pages to scrape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChromeDriver is working!\n"
     ]
    }
   ],
   "source": [
    "# If there's errors in the driver try doing this in the terminal and then running the commented code below\n",
    "# rm -rf ~/.wdm/drivers/chromedriver/ (deletes the cached driver)\n",
    "'''\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "# Force webdriver-manager to get the latest compatible driver\n",
    "service = Service(ChromeDriverManager().install())\n",
    "driver = webdriver.Chrome(service=service)\n",
    "\n",
    "print(\"ChromeDriver is working!\")\n",
    "driver.quit()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Found the number of items on the page by looking at the url pattern. In this case the 'start' changed by 15 for each page. I.e. the view is limited to 15 items per page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 1...\n"
     ]
    }
   ],
   "source": [
    "# Set up Selenium WebDriver using ChromeDriverManager (A WebDriver is a tool used to automate web browsers. It allows programs to control a browser)\n",
    "service = Service(ChromeDriverManager().install())\n",
    "driver = webdriver.Chrome(service=service)\n",
    "\n",
    "# Define the base URL with {} placeholder to insert the page number dynamically\n",
    "# Account Holders\n",
    "#npage=6\n",
    "#base_url = \"https://carbonregistry.gov.bc.ca/br-reg/public/bc/index.jsp?entity=account&name=&standardId=&acronym=&unitClass=&sort=account_name&dir=ASC&start={}\"\n",
    "# Projects\n",
    "#npage = 2\n",
    "#base_url = \"https://carbonregistry.gov.bc.ca/br-reg/public/bc/index.jsp?entity=project&name=&standardId=&acronym=&unitClass=&sort=project_name&dir=ASC&start={}\"\n",
    "# Issuances \n",
    "#npage = 8\n",
    "#base_url = \"https://carbonregistry.gov.bc.ca/br-reg/public/bc/index.jsp?entity=issuance&name=&standardId=&acronym=&unitClass=&sort=account_name&dir=ASC&start={}\"\n",
    "# Holdings\n",
    "npage=6\n",
    "base_url = \"https://carbonregistry.gov.bc.ca/br-reg/public/bc/index.jsp?entity=holding&name=&standardId=&acronym=&unitClass=&sort=account_name&dir=ASC&start={}\"\n",
    "# Retirements\n",
    "#base_url = \"https://carbonregistry.gov.bc.ca/br-reg/public/bc/index.jsp?entity=retirement&name=&standardId=&acronym=&unitClass=&sort=retirement_date&dir=ASC&start={}\"\n",
    "# Cancellations\n",
    "#base_url = \"https://carbonregistry.gov.bc.ca/br-reg/public/bc/index.jsp?entity=Cancelled&name=&standardId=&acronym=&unitClass=&sort=retirement_date&dir=ASC&start={}\"\n",
    "\n",
    "# Function to scrape data from one page\n",
    "def scrape_page(page_num):\n",
    "    # Create the URL for the given page number\n",
    "    url = base_url.format(page_num * 15)\n",
    "    \n",
    "    # Navigate to the page using Selenium\n",
    "    driver.get(url)\n",
    "    \n",
    "    # Wait for the page to load\n",
    "    time.sleep(10)  # Adjust if necessary for slower pages\n",
    "    \n",
    "    # Get the page source after JavaScript has loaded\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    \n",
    "    # Find the table\n",
    "    table = soup.find('table')  # Look for the table tag\n",
    "\n",
    "    if table is None:\n",
    "        print(f\"Table not found on page {page_num + 1}\")\n",
    "        return []\n",
    "\n",
    "    # Extract all rows in the table\n",
    "    rows = table.find_all('tr')[1:]  # Skip the header row\n",
    "    \n",
    "    # Create a list to store the table data\n",
    "    data = []\n",
    "    \n",
    "    # Loop through each row and extract the cell data\n",
    "    for row in rows:\n",
    "        cells = row.find_all('td')\n",
    "        row_data = [cell.get_text(strip=True) for cell in cells]\n",
    "        data.append(row_data)\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Scrape the first 5 pages (adjust the range to scrape more)\n",
    "all_data = []\n",
    "for page in range(npage):  # Change to 65 for full scraping\n",
    "    print(f\"Scraping page {page + 1}...\")\n",
    "    page_data = scrape_page(page)\n",
    "    if page_data:\n",
    "        all_data.extend(page_data)\n",
    "\n",
    "# Close the browser\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tab_var = 'holdings'\n",
    "# Account Holders\n",
    "#cols=['Account Name', 'Classification', 'Website']\n",
    "# Projects\n",
    "#cols=['Name', 'Project Type', 'Status', 'Validator', 'Proponent', 'Details']\n",
    "# Issuances / Listings\n",
    "# cols=[\"Vintage\", \"Project\", \"Account\", \"Project Type\", \"Verifier\", \"Units\", 'Measurement', \"Details\"]\n",
    "# Holdings\n",
    "cols=[\"Vintage\", \"Project\", \"Account\", \"Standard\", \"Project Type\", \"Verifier\", \"Units\", 'Measurement', 'Type', \"Details\"]\n",
    "# Retirements\n",
    "#cols=[\"Retirement Date\", \"Vintage\", \"Project\", \"Account\", \"Project Type\", \"Retirement Quantity\", 'Measurement', 'Details']\n",
    "# Cancellations\n",
    "#cols=[\"Cancellation Date\", \"Vintage\", \"Project\", 'Account',\"Standard\", \"Project Type\", \"Cancellation Quantity\", 'Measurement', 'Type','Details']\n",
    "df = pd.DataFrame(all_data, columns=cols)\n",
    "df.to_csv(f'../data/offset_registries/BCcarbon_registry/BCcarbon_registry_{tab_var}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
